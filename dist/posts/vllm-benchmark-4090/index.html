<!DOCTYPE html><html lang="en" class="scroll-smooth"> <head><!-- Google tag (gtag.js) --><script type="text/partytown" async src="https://www.googletagmanager.com/gtag/js?id=G-8MLZNVYK1Z"></script><script type="text/partytown">
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-8MLZNVYK1Z");
    </script><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><link rel="canonical" href="https://ermolushka.github.io/posts/vllm-benchmark-4090/"><meta name="generator" content="Astro v4.16.3"><!-- General Meta Tags --><title>Memory Optimization Deep Dive Running 8B Models on a Single 4090 using vLLM | ermolushka (blog)</title><meta name="title" content="Memory Optimization Deep Dive Running 8B Models on a Single 4090 using vLLM | ermolushka (blog)"><meta name="description" content="An exploration of quantization techniques and memory optimization strategies for running Llama 8B models efficiently on consumer hardware using vLLM"><meta name="author" content="Alexey Ermolaev"><link rel="sitemap" href="/sitemap-index.xml"><!-- Open Graph / Facebook --><meta property="og:title" content="Memory Optimization Deep Dive Running 8B Models on a Single 4090 using vLLM | ermolushka (blog)"><meta property="og:description" content="An exploration of quantization techniques and memory optimization strategies for running Llama 8B models efficiently on consumer hardware using vLLM"><meta property="og:url" content="https://ermolushka.github.io/posts/vllm-benchmark-4090/"><meta property="og:image" content="https://ermolushka.github.io/posts/memory-optimization-deep-dive-running-8-b-models-on-a-single-4090-using-v-llm.png"><!-- Article Published/Modified time --><meta property="article:published_time" content="2025-09-10T15:01:00.000Z"><meta property="article:modified_time" content="2025-09-10T15:01:00.000Z"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://ermolushka.github.io/posts/vllm-benchmark-4090/"><meta property="twitter:title" content="Memory Optimization Deep Dive Running 8B Models on a Single 4090 using vLLM | ermolushka (blog)"><meta property="twitter:description" content="An exploration of quantization techniques and memory optimization strategies for running Llama 8B models efficiently on consumer hardware using vLLM"><meta property="twitter:image" content="https://ermolushka.github.io/posts/memory-optimization-deep-dive-running-8-b-models-on-a-single-4090-using-v-llm.png"><!-- Google JSON-LD Structured data --><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Memory Optimization Deep Dive Running 8B Models on a Single 4090 using vLLM | ermolushka (blog)","image":"https://ermolushka.github.io/posts/memory-optimization-deep-dive-running-8-b-models-on-a-single-4090-using-v-llm.png","datePublished":"2025-09-10T15:01:00.000Z","dateModified":"2025-09-10T15:01:00.000Z","author":[{"@type":"Person","name":"Alexey Ermolaev","url":"https://github.com/ermolushka"}]}</script><!-- Google Font --><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:ital,wght@0,400;0,500;0,600;0,700;1,400;1,600&display=swap" rel="preload" as="style" onload="this.onload=null; this.rel='stylesheet';" crossorigin><meta name="theme-color" content=""><meta name="astro-view-transitions-enabled" content="true"><meta name="astro-view-transitions-fallback" content="animate"><script src="/toggle-theme.js" async></script><link rel="stylesheet" href="/_astro/about.Cp91FjQY.css">
<style>a:where(.astro-blwjyjpt){position:relative;text-decoration-line:underline;text-decoration-style:dashed}a:where(.astro-blwjyjpt):hover{top:-.125rem;--tw-text-opacity: 1;color:rgba(var(--color-accent),var(--tw-text-opacity))}a:where(.astro-blwjyjpt):focus-visible{padding:.25rem}a:where(.astro-blwjyjpt) svg:where(.astro-blwjyjpt){margin-right:-1.25rem;height:1.5rem;width:1.5rem;--tw-scale-x: .95;--tw-scale-y: .95;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y));--tw-text-opacity: 1;color:rgba(var(--color-text-base),var(--tw-text-opacity));opacity:.8}.group:where(.astro-blwjyjpt):hover a:where(.astro-blwjyjpt) svg:where(.astro-blwjyjpt){fill:rgb(var(--color-accent))}
@keyframes astroFadeInOut{0%{opacity:1}to{opacity:0}}@keyframes astroFadeIn{0%{opacity:0;mix-blend-mode:plus-lighter}to{opacity:1;mix-blend-mode:plus-lighter}}@keyframes astroFadeOut{0%{opacity:1;mix-blend-mode:plus-lighter}to{opacity:0;mix-blend-mode:plus-lighter}}@keyframes astroSlideFromRight{0%{transform:translate(100%)}}@keyframes astroSlideFromLeft{0%{transform:translate(-100%)}}@keyframes astroSlideToRight{to{transform:translate(100%)}}@keyframes astroSlideToLeft{to{transform:translate(-100%)}}@media (prefers-reduced-motion){::view-transition-group(*),::view-transition-old(*),::view-transition-new(*){animation:none!important}[data-astro-transition-scope]{animation:none!important}}
.social-icons:where(.astro-wkojbtzc){display:flex;flex-direction:column;flex-wrap:wrap;align-items:center;justify-content:center;gap:.25rem}@media (min-width: 640px){.social-icons:where(.astro-wkojbtzc){align-items:flex-start}}.link-button:where(.astro-wkojbtzc){--tw-scale-x: .9;--tw-scale-y: .9;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y));padding:.5rem}.link-button:where(.astro-wkojbtzc):hover{--tw-rotate: 6deg;transform:translate(var(--tw-translate-x),var(--tw-translate-y)) rotate(var(--tw-rotate)) skew(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y))}@media (min-width: 640px){.link-button:where(.astro-wkojbtzc){padding:.25rem}}main:where(.astro-vj4tpspi){margin-left:auto;margin-right:auto;width:100%;max-width:48rem;padding-left:1rem;padding-right:1rem;padding-bottom:3rem}.post-title:where(.astro-vj4tpspi){font-size:1.5rem;line-height:2rem;font-weight:600;--tw-text-opacity: 1;color:rgba(var(--color-accent),var(--tw-text-opacity))}
</style><script type="module" src="/_astro/hoisted.BYHvv9Tn.js"></script>
<script>!(function(w,p,f,c){if(!window.crossOriginIsolated && !navigator.serviceWorker) return;c=w[p]=Object.assign(w[p]||{},{"lib":"/~partytown/","debug":false});c[f]=(c[f]||[]).concat(["dataLayer.push"])})(window,'partytown','forward');/* Partytown 0.11.0 - MIT QwikDev */
const t={preserveBehavior:!1},e=e=>{if("string"==typeof e)return[e,t];const[n,r=t]=e;return[n,{...t,...r}]},n=Object.freeze((t=>{const e=new Set;let n=[];do{Object.getOwnPropertyNames(n).forEach((t=>{"function"==typeof n[t]&&e.add(t)}))}while((n=Object.getPrototypeOf(n))!==Object.prototype);return Array.from(e)})());!function(t,r,o,i,a,s,c,l,d,p,u=t,f){function h(){f||(f=1,"/"==(c=(s.lib||"/~partytown/")+(s.debug?"debug/":""))[0]&&(d=r.querySelectorAll('script[type="text/partytown"]'),i!=t?i.dispatchEvent(new CustomEvent("pt1",{detail:t})):(l=setTimeout(v,(null==s?void 0:s.fallbackTimeout)||1e4),r.addEventListener("pt0",w),a?y(1):o.serviceWorker?o.serviceWorker.register(c+(s.swPath||"partytown-sw.js"),{scope:c}).then((function(t){t.active?y():t.installing&&t.installing.addEventListener("statechange",(function(t){"activated"==t.target.state&&y()}))}),console.error):v())))}function y(e){p=r.createElement(e?"script":"iframe"),t._pttab=Date.now(),e||(p.style.display="block",p.style.width="0",p.style.height="0",p.style.border="0",p.style.visibility="hidden",p.setAttribute("aria-hidden",!0)),p.src=c+"partytown-"+(e?"atomics.js?v=0.11.0":"sandbox-sw.html?"+t._pttab),r.querySelector(s.sandboxParent||"body").appendChild(p)}function v(n,o){for(w(),i==t&&(s.forward||[]).map((function(n){const[r]=e(n);delete t[r.split(".")[0]]})),n=0;n<d.length;n++)(o=r.createElement("script")).innerHTML=d[n].innerHTML,o.nonce=s.nonce,r.head.appendChild(o);p&&p.parentNode.removeChild(p)}function w(){clearTimeout(l)}s=t.partytown||{},i==t&&(s.forward||[]).map((function(r){const[o,{preserveBehavior:i}]=e(r);u=t,o.split(".").map((function(e,r,o){var a;u=u[o[r]]=r+1<o.length?u[o[r]]||(a=o[r+1],n.includes(a)?[]:{}):(()=>{let e=null;if(i){const{methodOrProperty:n,thisObject:r}=((t,e)=>{let n=t;for(let t=0;t<e.length-1;t+=1)n=n[e[t]];return{thisObject:n,methodOrProperty:e.length>0?n[e[e.length-1]]:void 0}})(t,o);"function"==typeof n&&(e=(...t)=>n.apply(r,...t))}return function(){let n;return e&&(n=e(arguments)),(t._ptf=t._ptf||[]).push(o,arguments),n}})()}))})),"complete"==r.readyState?h():(t.addEventListener("DOMContentLoaded",h),t.addEventListener("load",h))}(window,document,navigator,top,window.crossOriginIsolated);;(e=>{e.addEventListener("astro:before-swap",e=>{let r=document.body.querySelector("iframe[src*='/~partytown/']");if(r)e.newDocument.body.append(r)})})(document);</script><style>[data-astro-transition-scope="astro-plk3gbjq-1"] { view-transition-name: memory-optimization-deep-dive-running-8-b-models-on-a-single-4090-using-v-llm; }@layer astro { ::view-transition-old(memory-optimization-deep-dive-running-8-b-models-on-a-single-4090-using-v-llm) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }::view-transition-new(memory-optimization-deep-dive-running-8-b-models-on-a-single-4090-using-v-llm) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back]::view-transition-old(memory-optimization-deep-dive-running-8-b-models-on-a-single-4090-using-v-llm) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back]::view-transition-new(memory-optimization-deep-dive-running-8-b-models-on-a-single-4090-using-v-llm) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; } }[data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-plk3gbjq-1"],
			[data-astro-transition-fallback="old"][data-astro-transition-scope="astro-plk3gbjq-1"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-plk3gbjq-1"],
			[data-astro-transition-fallback="new"][data-astro-transition-scope="astro-plk3gbjq-1"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back][data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-plk3gbjq-1"],
			[data-astro-transition=back][data-astro-transition-fallback="old"][data-astro-transition-scope="astro-plk3gbjq-1"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back][data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-plk3gbjq-1"],
			[data-astro-transition=back][data-astro-transition-fallback="new"][data-astro-transition-scope="astro-plk3gbjq-1"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }</style><style>[data-astro-transition-scope="astro-36ssibgs-2"] { view-transition-name: v-llm; }@layer astro { ::view-transition-old(v-llm) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }::view-transition-new(v-llm) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back]::view-transition-old(v-llm) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back]::view-transition-new(v-llm) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; } }[data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-36ssibgs-2"],
			[data-astro-transition-fallback="old"][data-astro-transition-scope="astro-36ssibgs-2"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-36ssibgs-2"],
			[data-astro-transition-fallback="new"][data-astro-transition-scope="astro-36ssibgs-2"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back][data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-36ssibgs-2"],
			[data-astro-transition=back][data-astro-transition-fallback="old"][data-astro-transition-scope="astro-36ssibgs-2"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back][data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-36ssibgs-2"],
			[data-astro-transition=back][data-astro-transition-fallback="new"][data-astro-transition-scope="astro-36ssibgs-2"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }</style><style>[data-astro-transition-scope="astro-36ssibgs-3"] { view-transition-name: ml-inference; }@layer astro { ::view-transition-old(ml-inference) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }::view-transition-new(ml-inference) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back]::view-transition-old(ml-inference) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back]::view-transition-new(ml-inference) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; } }[data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-36ssibgs-3"],
			[data-astro-transition-fallback="old"][data-astro-transition-scope="astro-36ssibgs-3"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-36ssibgs-3"],
			[data-astro-transition-fallback="new"][data-astro-transition-scope="astro-36ssibgs-3"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back][data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-36ssibgs-3"],
			[data-astro-transition=back][data-astro-transition-fallback="old"][data-astro-transition-scope="astro-36ssibgs-3"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back][data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-36ssibgs-3"],
			[data-astro-transition=back][data-astro-transition-fallback="new"][data-astro-transition-scope="astro-36ssibgs-3"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }</style><style>[data-astro-transition-scope="astro-36ssibgs-4"] { view-transition-name: cuda; }@layer astro { ::view-transition-old(cuda) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }::view-transition-new(cuda) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back]::view-transition-old(cuda) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back]::view-transition-new(cuda) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; } }[data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-36ssibgs-4"],
			[data-astro-transition-fallback="old"][data-astro-transition-scope="astro-36ssibgs-4"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-36ssibgs-4"],
			[data-astro-transition-fallback="new"][data-astro-transition-scope="astro-36ssibgs-4"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back][data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-36ssibgs-4"],
			[data-astro-transition=back][data-astro-transition-fallback="old"][data-astro-transition-scope="astro-36ssibgs-4"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back][data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-36ssibgs-4"],
			[data-astro-transition=back][data-astro-transition-fallback="new"][data-astro-transition-scope="astro-36ssibgs-4"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }</style><style>[data-astro-transition-scope="astro-36ssibgs-5"] { view-transition-name: gpu; }@layer astro { ::view-transition-old(gpu) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }::view-transition-new(gpu) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back]::view-transition-old(gpu) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back]::view-transition-new(gpu) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; } }[data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-36ssibgs-5"],
			[data-astro-transition-fallback="old"][data-astro-transition-scope="astro-36ssibgs-5"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-36ssibgs-5"],
			[data-astro-transition-fallback="new"][data-astro-transition-scope="astro-36ssibgs-5"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back][data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-36ssibgs-5"],
			[data-astro-transition=back][data-astro-transition-fallback="old"][data-astro-transition-scope="astro-36ssibgs-5"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back][data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-36ssibgs-5"],
			[data-astro-transition=back][data-astro-transition-fallback="new"][data-astro-transition-scope="astro-36ssibgs-5"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }</style><style>[data-astro-transition-scope="astro-36ssibgs-6"] { view-transition-name: llama; }@layer astro { ::view-transition-old(llama) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }::view-transition-new(llama) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back]::view-transition-old(llama) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back]::view-transition-new(llama) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; } }[data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-36ssibgs-6"],
			[data-astro-transition-fallback="old"][data-astro-transition-scope="astro-36ssibgs-6"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-36ssibgs-6"],
			[data-astro-transition-fallback="new"][data-astro-transition-scope="astro-36ssibgs-6"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back][data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-36ssibgs-6"],
			[data-astro-transition=back][data-astro-transition-fallback="old"][data-astro-transition-scope="astro-36ssibgs-6"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back][data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-36ssibgs-6"],
			[data-astro-transition=back][data-astro-transition-fallback="new"][data-astro-transition-scope="astro-36ssibgs-6"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }</style><style>[data-astro-transition-scope="astro-36ssibgs-7"] { view-transition-name: quantization; }@layer astro { ::view-transition-old(quantization) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }::view-transition-new(quantization) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back]::view-transition-old(quantization) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back]::view-transition-new(quantization) { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; } }[data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-36ssibgs-7"],
			[data-astro-transition-fallback="old"][data-astro-transition-scope="astro-36ssibgs-7"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-36ssibgs-7"],
			[data-astro-transition-fallback="new"][data-astro-transition-scope="astro-36ssibgs-7"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }[data-astro-transition=back][data-astro-transition-fallback="old"] [data-astro-transition-scope="astro-36ssibgs-7"],
			[data-astro-transition=back][data-astro-transition-fallback="old"][data-astro-transition-scope="astro-36ssibgs-7"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeOut; }[data-astro-transition=back][data-astro-transition-fallback="new"] [data-astro-transition-scope="astro-36ssibgs-7"],
			[data-astro-transition=back][data-astro-transition-fallback="new"][data-astro-transition-scope="astro-36ssibgs-7"] { 
	animation-duration: 180ms;
	animation-timing-function: cubic-bezier(0.76, 0, 0.24, 1);
	animation-fill-mode: both;
	animation-name: astroFadeIn; }</style></head> <body>  <header class="astro-3ef6ksr2"> <a id="skip-to-content" href="#main-content" class="astro-3ef6ksr2">Skip to content</a> <div class="nav-container astro-3ef6ksr2"> <div class="top-nav-wrap astro-3ef6ksr2"> <a href="/" class="logo whitespace-nowrap astro-3ef6ksr2"> ermolushka (blog) </a> <nav id="nav-menu" class="astro-3ef6ksr2"> <button class="hamburger-menu focus-outline astro-3ef6ksr2" aria-label="Open Menu" aria-expanded="false" aria-controls="menu-items"> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="menu-icon astro-3ef6ksr2"> <line x1="7" y1="12" x2="21" y2="12" class="line astro-3ef6ksr2"></line> <line x1="3" y1="6" x2="21" y2="6" class="line astro-3ef6ksr2"></line> <line x1="12" y1="18" x2="21" y2="18" class="line astro-3ef6ksr2"></line> <line x1="18" y1="6" x2="6" y2="18" class="close astro-3ef6ksr2"></line> <line x1="6" y1="6" x2="18" y2="18" class="close astro-3ef6ksr2"></line> </svg> </button> <ul id="menu-items" class="display-none sm:flex astro-3ef6ksr2"> <li class="astro-3ef6ksr2"> <a href="/posts/" class=" astro-3ef6ksr2">
Posts
</a> </li> <li class="astro-3ef6ksr2"> <a href="/tags/" class=" astro-3ef6ksr2">
Tags
</a> </li> <li class="astro-3ef6ksr2"> <a href="/about/" class=" astro-3ef6ksr2">
About
</a> </li> <li class="astro-3ef6ksr2"> <a href="/archives/" class="group inline-block hover:text-skin-accent focus-outline flex justify-center p-3 sm:p-1 astro-3ef6ksr2" aria-label="archives" title="Archives"> <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icons-tabler-outline !hidden sm:!inline-block astro-3ef6ksr2">  <path stroke="none" d="M0 0h24v24H0z" fill="none" class="astro-3ef6ksr2"></path> <path d="M3 4m0 2a2 2 0 0 1 2 -2h14a2 2 0 0 1 2 2v0a2 2 0 0 1 -2 2h-14a2 2 0 0 1 -2 -2z" class="astro-3ef6ksr2"></path> <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" class="astro-3ef6ksr2"></path> <path d="M10 12l4 0" class="astro-3ef6ksr2"></path>  </svg> <span class="sm:sr-only astro-3ef6ksr2">
Archives
</span> </a> </li> <li class="astro-3ef6ksr2"> <a href="/search/" class="group inline-block hover:text-skin-accent focus-outline p-3 sm:p-1  flex astro-3ef6ksr2" aria-label="search" title="Search"> <svg xmlns="http://www.w3.org/2000/svg" class="scale-125 sm:scale-100 astro-3ef6ksr2"><path d="M19.023 16.977a35.13 35.13 0 0 1-1.367-1.384c-.372-.378-.596-.653-.596-.653l-2.8-1.337A6.962 6.962 0 0 0 16 9c0-3.859-3.14-7-7-7S2 5.141 2 9s3.14 7 7 7c1.763 0 3.37-.66 4.603-1.739l1.337 2.8s.275.224.653.596c.387.363.896.854 1.384 1.367l1.358 1.392.604.646 2.121-2.121-.646-.604c-.379-.372-.885-.866-1.391-1.36zM9 14c-2.757 0-5-2.243-5-5s2.243-5 5-5 5 2.243 5 5-2.243 5-5 5z" class="astro-3ef6ksr2"></path> </svg> <span class="sr-only astro-3ef6ksr2">Search</span> </a> </li> <li class="astro-3ef6ksr2"> <button id="theme-btn" class="focus-outline astro-3ef6ksr2" title="Toggles light & dark" aria-label="auto" aria-live="polite"> <svg xmlns="http://www.w3.org/2000/svg" id="moon-svg" class="astro-3ef6ksr2"> <path d="M20.742 13.045a8.088 8.088 0 0 1-2.077.271c-2.135 0-4.14-.83-5.646-2.336a8.025 8.025 0 0 1-2.064-7.723A1 1 0 0 0 9.73 2.034a10.014 10.014 0 0 0-4.489 2.582c-3.898 3.898-3.898 10.243 0 14.143a9.937 9.937 0 0 0 7.072 2.93 9.93 9.93 0 0 0 7.07-2.929 10.007 10.007 0 0 0 2.583-4.491 1.001 1.001 0 0 0-1.224-1.224zm-2.772 4.301a7.947 7.947 0 0 1-5.656 2.343 7.953 7.953 0 0 1-5.658-2.344c-3.118-3.119-3.118-8.195 0-11.314a7.923 7.923 0 0 1 2.06-1.483 10.027 10.027 0 0 0 2.89 7.848 9.972 9.972 0 0 0 7.848 2.891 8.036 8.036 0 0 1-1.484 2.059z" class="astro-3ef6ksr2"></path> </svg> <svg xmlns="http://www.w3.org/2000/svg" id="sun-svg" class="astro-3ef6ksr2"> <path d="M6.993 12c0 2.761 2.246 5.007 5.007 5.007s5.007-2.246 5.007-5.007S14.761 6.993 12 6.993 6.993 9.239 6.993 12zM12 8.993c1.658 0 3.007 1.349 3.007 3.007S13.658 15.007 12 15.007 8.993 13.658 8.993 12 10.342 8.993 12 8.993zM10.998 19h2v3h-2zm0-17h2v3h-2zm-9 9h3v2h-3zm17 0h3v2h-3zM4.219 18.363l2.12-2.122 1.415 1.414-2.12 2.122zM16.24 6.344l2.122-2.122 1.414 1.414-2.122 2.122zM6.342 7.759 4.22 5.637l1.415-1.414 2.12 2.122zm13.434 10.605-1.414 1.414-2.122-2.122 1.414-1.414z" class="astro-3ef6ksr2"></path> </svg> </button> </li> </ul> </nav> </div> </div> <div class="max-w-3xl mx-auto px-4"> <hr class="border-skin-line" aria-hidden="true"> </div> </header>   <div class="mx-auto flex w-full max-w-3xl justify-start px-2 astro-vj4tpspi"> <button class="focus-outline mb-2 mt-8 flex hover:opacity-75 astro-vj4tpspi" onclick="(() => (history.length === 1) ? window.location = '/' : history.back())()"> <svg xmlns="http://www.w3.org/2000/svg" class="astro-vj4tpspi"><path d="M13.293 6.293 7.586 12l5.707 5.707 1.414-1.414L10.414 12l4.293-4.293z" class="astro-vj4tpspi"></path> </svg><span class="astro-vj4tpspi">Go back</span> </button> </div> <main id="main-content" class="astro-vj4tpspi"> <h1 class="post-title astro-vj4tpspi" data-astro-transition-scope="astro-plk3gbjq-1">Memory Optimization Deep Dive Running 8B Models on a Single 4090 using vLLM</h1> <div class="flex items-center space-x-2 opacity-80 my-2 astro-vj4tpspi"><svg xmlns="http://www.w3.org/2000/svg" class="scale-100 inline-block h-6 w-6 min-w-[1.375rem] fill-skin-base" aria-hidden="true"><path d="M7 11h2v2H7zm0 4h2v2H7zm4-4h2v2h-2zm0 4h2v2h-2zm4-4h2v2h-2zm0 4h2v2h-2z"></path><path d="M5 22h14c1.103 0 2-.897 2-2V6c0-1.103-.897-2-2-2h-2V2h-2v2H9V2H7v2H5c-1.103 0-2 .897-2 2v14c0 1.103.897 2 2 2zM19 8l.001 12H5V8h14z"></path></svg><span class="sr-only">Published:</span><span class="italic text-base"><time dateTime="2025-09-10T15:01:00.000Z">Sep 10, 2025</time><span aria-hidden="true"> | </span><span class="sr-only"> at </span><span class="text-nowrap">08:01 AM</span></span></div> <article id="article" class="prose mx-auto mt-8 max-w-3xl astro-vj4tpspi"> <h2 id="introduction">Introduction</h2>
<p>Running large language models like Llama 8B (8 billion parameters) on consumer hardware presents challenges around GPU memory constraints. The RTX 4090, despite its 24GB of VRAM, requires careful optimization to run these models effectively while maintaining good performance and quality.</p>
<p>This experiment examines different quantization techniques and memory optimization strategies, providing concrete benchmarks and insights for running 8B models on a single RTX 4090.</p>
<h2 id="the-memory-challenge">The Memory Challenge</h2>
<h3 id="understanding-model-memory-requirements">Understanding Model Memory Requirements</h3>
<p>A typical Llama 8B model in FP16 precision requires approximately:</p>
<ul>
<li><strong>Model Weights</strong>: 8B × 2 bytes = 16GB</li>
<li><strong>KV Cache</strong>: Variable, depends on context length and batch size</li>
<li><strong>Activation Memory</strong>: ~2-4GB during inference</li>
<li><strong>Framework Overhead</strong>: ~1-2GB</li>
</ul>
<p>This puts us right at the edge of what a 24GB RTX 4090 can handle, leaving little room for longer contexts or batch processing.</p>
<p><strong>Note</strong>: The actual 22.6GB usage measured in my experiments includes KV cache, activations, and framework overhead, which are larger than the estimated 3-5GB due to vLLM’s memory management and pre-allocation strategies.</p>
<h2 id="experimental-setup">Experimental Setup</h2>
<h3 id="hardware-configuration">Hardware Configuration</h3>
<ul>
<li><strong>GPU</strong>: NVIDIA RTX 4090 (24GB VRAM, Ada Lovelace architecture)</li>
<li><strong>CPU</strong>: AMD/Intel CPU with sufficient cores</li>
<li><strong>RAM</strong>: 32GB DDR4/DDR5 system memory</li>
<li><strong>CUDA</strong>: 13.0</li>
<li><strong>Driver</strong>: NVIDIA driver (581.15)</li>
</ul>
<h3 id="software-stack">Software Stack</h3>
<ul>
<li><strong>vLLM</strong>: v0.10.1.1 (for FP16, AWQ, GPTQ experiments)</li>
<li><strong>Transformers</strong>: v4.56.1 (for BitsAndBytes experiments)</li>
<li><strong>BitsAndBytes</strong>: v0.47.0 (for 4-bit quantization)</li>
<li><strong>AutoAWQ</strong>: v0.2.9 (for AWQ quantization)</li>
<li><strong>AutoGPTQ</strong>: v0.7.1 (for GPTQ quantization)</li>
</ul>
<h3 id="benchmarking-methodology">Benchmarking Methodology</h3>
<p>Each experiment follows a standardized protocol:</p>
<ol>
<li><strong>Memory Baseline</strong>: Record initial GPU memory state</li>
<li><strong>Model Loading</strong>: Load model and measure memory increase</li>
<li><strong>Inference Test</strong>: Run 7 representative prompts (256 tokens each)</li>
<li><strong>Performance Metrics</strong>: Measure tokens/second, memory usage, load time</li>
</ol>
<h2 id="experiment-1-fp16-baseline">Experiment 1: FP16 Baseline</h2>
<p><em>Establishing performance and memory baseline with standard FP16 precision.</em></p>
<h3 id="configuration">Configuration</h3>
<pre class="astro-code astro-code-themes min-light night-owl" style="background-color:#ffffff;--shiki-dark-bg:#011627;color:#24292eff;--shiki-dark:#d6deeb; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#24292EFF;--shiki-dark:#D6DEEB">llm </span><span style="color:#D32F2F;--shiki-dark:#C792EA">=</span><span style="color:#6F42C1;--shiki-dark:#B2CCD6"> LLM</span><span style="color:#212121;--shiki-dark:#D6DEEB">(</span></span>
<span class="line"><span style="color:#212121;--shiki-dark:#D7DBE0">   model</span><span style="color:#D32F2F;--shiki-dark:#C792EA">=</span><span style="color:#22863A;--shiki-dark:#D9F5DD">"</span><span style="color:#22863A;--shiki-dark:#ECC48D">meta-llama/Meta-Llama-3.1-8B-Instruct</span><span style="color:#22863A;--shiki-dark:#D9F5DD">"</span><span style="color:#212121;--shiki-dark:#D9F5DD">,</span></span>
<span class="line"><span style="color:#212121;--shiki-dark:#D7DBE0">   dtype</span><span style="color:#D32F2F;--shiki-dark:#C792EA">=</span><span style="color:#22863A;--shiki-dark:#D9F5DD">"</span><span style="color:#22863A;--shiki-dark:#ECC48D">float16</span><span style="color:#22863A;--shiki-dark:#D9F5DD">"</span><span style="color:#212121;--shiki-dark:#D9F5DD">,</span></span>
<span class="line"><span style="color:#212121;--shiki-dark:#D7DBE0">   tensor_parallel_size</span><span style="color:#D32F2F;--shiki-dark:#C792EA">=</span><span style="color:#1976D2;--shiki-dark:#F78C6C">1</span><span style="color:#212121;--shiki-dark:#D9F5DD">,</span></span>
<span class="line"><span style="color:#212121;--shiki-dark:#D7DBE0">   gpu_memory_utilization</span><span style="color:#D32F2F;--shiki-dark:#C792EA">=</span><span style="color:#1976D2;--shiki-dark:#F78C6C">0.9</span><span style="color:#212121;--shiki-dark:#D9F5DD">,</span></span>
<span class="line"><span style="color:#212121;--shiki-dark:#D7DBE0">   max_model_len</span><span style="color:#D32F2F;--shiki-dark:#C792EA">=</span><span style="color:#1976D2;--shiki-dark:#F78C6C">4096</span><span style="color:#212121;--shiki-dark:#D9F5DD">,</span></span>
<span class="line"><span style="color:#212121;--shiki-dark:#D7DBE0">   enforce_eager</span><span style="color:#D32F2F;--shiki-dark:#C792EA">=</span><span style="color:#1976D2;--shiki-dark:#FF5874">True</span><span style="color:#212121;--shiki-dark:#D9F5DD">,</span><span style="color:#C2C3C5;--shiki-dark:#637777">  #</span><span style="color:#C2C3C5;font-style:inherit;--shiki-dark:#637777;--shiki-dark-font-style:italic"> Disable CUDA graphs for more consistent memory measurement</span></span>
<span class="line"><span style="color:#212121;--shiki-dark:#D6DEEB">)</span></span>
<span class="line"></span></code></pre>
<h3 id="results">Results</h3>





























<table><thead><tr><th>Metric</th><th>Value</th></tr></thead><tbody><tr><td>GPU Memory Used</td><td>22,631MB (92.1% of VRAM)</td></tr><tr><td>Load Time</td><td>16.1s</td></tr><tr><td>Tokens/Second</td><td>339.6</td></tr><tr><td>Peak GPU Utilization</td><td>99.2%</td></tr><tr><td>Model Quality</td><td>Baseline</td></tr></tbody></table>
<h3 id="analysis">Analysis</h3>
<p>The FP16 baseline establishes performance reference point. As expected, the full-precision model consumes nearly all available VRAM on the RTX 4090, leaving only ~200MB free. The model loads in a reasonable 16 seconds and delivers solid inference performance at 339.6 tokens/second across my 7-prompt test suite.</p>
<p>This near-maximum VRAM usage (99.2%) demonstrates why quantization is essential for practical deployment - there’s virtually no headroom for longer contexts, batch processing, or other optimizations.</p>
<h2 id="experiment-2-bitsandbytes-4-bit-quantization">Experiment 2: BitsAndBytes 4-bit Quantization</h2>
<p><em>Testing the most accessible quantization method with an up-to-date tooling support.</em></p>
<h3 id="why-bitsandbytes">Why BitsAndBytes?</h3>
<p>BitsAndBytes offers several advantages:</p>
<ul>
<li><strong>Easy Integration</strong>: Works directly with Transformers library</li>
<li><strong>Quality Preservation</strong>: Uses advanced quantization schemes (NF4, double quantization)</li>
<li><strong>Flexible</strong>: Supports mixed precision and selective quantization</li>
</ul>
<h3 id="configuration-1">Configuration</h3>
<pre class="astro-code astro-code-themes min-light night-owl" style="background-color:#ffffff;--shiki-dark-bg:#011627;color:#24292eff;--shiki-dark:#d6deeb; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="python"><code><span class="line"><span style="color:#24292EFF;--shiki-dark:#D6DEEB">bnb_config </span><span style="color:#D32F2F;--shiki-dark:#C792EA">=</span><span style="color:#6F42C1;--shiki-dark:#B2CCD6"> BitsAndBytesConfig</span><span style="color:#212121;--shiki-dark:#D6DEEB">(</span></span>
<span class="line"><span style="color:#212121;--shiki-dark:#D7DBE0">   load_in_4bit</span><span style="color:#D32F2F;--shiki-dark:#C792EA">=</span><span style="color:#1976D2;--shiki-dark:#FF5874">True</span><span style="color:#212121;--shiki-dark:#D9F5DD">,</span></span>
<span class="line"><span style="color:#212121;--shiki-dark:#D7DBE0">   bnb_4bit_compute_dtype</span><span style="color:#D32F2F;--shiki-dark:#C792EA">=</span><span style="color:#212121;--shiki-dark:#82AAFF">torch.float16</span><span style="color:#212121;--shiki-dark:#D9F5DD">,</span></span>
<span class="line"><span style="color:#212121;--shiki-dark:#D7DBE0">   bnb_4bit_use_double_quant</span><span style="color:#D32F2F;--shiki-dark:#C792EA">=</span><span style="color:#1976D2;--shiki-dark:#FF5874">True</span><span style="color:#212121;--shiki-dark:#D9F5DD">,</span><span style="color:#C2C3C5;--shiki-dark:#637777">  #</span><span style="color:#C2C3C5;font-style:inherit;--shiki-dark:#637777;--shiki-dark-font-style:italic"> Nested quantization for additional memory savings</span></span>
<span class="line"><span style="color:#212121;--shiki-dark:#D7DBE0">   bnb_4bit_quant_type</span><span style="color:#D32F2F;--shiki-dark:#C792EA">=</span><span style="color:#22863A;--shiki-dark:#D9F5DD">"</span><span style="color:#22863A;--shiki-dark:#ECC48D">nf4</span><span style="color:#22863A;--shiki-dark:#D9F5DD">"</span><span style="color:#212121;--shiki-dark:#D9F5DD">,</span><span style="color:#C2C3C5;--shiki-dark:#637777">  #</span><span style="color:#C2C3C5;font-style:inherit;--shiki-dark:#637777;--shiki-dark-font-style:italic"> Normal Float 4-bit quantization</span></span>
<span class="line"><span style="color:#212121;--shiki-dark:#D6DEEB">)</span></span>
<span class="line"></span></code></pre>
<h3 id="results-1">Results</h3>



































<table><thead><tr><th>Metric</th><th>Value</th><th>vs FP16</th></tr></thead><tbody><tr><td>GPU Memory Used</td><td>7,711MB (31.4% of VRAM)</td><td>66% reduction</td></tr><tr><td>Load Time</td><td>6.6s</td><td>59% faster</td></tr><tr><td>Tokens/Second</td><td>42.4</td><td>87% slower</td></tr><tr><td>Peak GPU Utilization</td><td>38.5%</td><td>61% lower</td></tr><tr><td>Memory Savings</td><td>14.9GB</td><td>-</td></tr></tbody></table>
<h3 id="memory-usage-breakdown">Memory Usage Breakdown</h3>
<p>BitsAndBytes delivers exceptional memory optimization results:</p>
<ul>
<li><strong>Model footprint</strong>: Reduced from 22.6GB to 7.7GB</li>
<li><strong>Free VRAM</strong>: 15.1GB available (vs 0.2GB with FP16)</li>
<li><strong>Memory efficiency</strong>: 66% reduction in GPU memory usage</li>
</ul>
<p>This dramatic improvement opens up possibilities for:</p>
<ul>
<li>Longer context lengths (up to ~32K tokens estimated)</li>
<li>Batch processing multiple requests</li>
<li>Running additional models simultaneously</li>
</ul>
<h3 id="quality-impact">Quality Impact</h3>
<p>While BitsAndBytes achieves outstanding memory savings, it comes with a significant performance trade-off. Inference speed drops to 42.4 tokens/second (87% slower than FP16). However, the quality of outputs remains good thanks to:</p>
<ul>
<li><strong>NF4 quantization</strong>: Optimized for neural network weights</li>
<li><strong>Double quantization</strong>: Further compression without major quality loss</li>
<li><strong>FP16 compute</strong>: Maintains precision during computation</li>
</ul>
<h2 id="experiment-3-awq-activation-aware-weight-quantization">Experiment 3: AWQ (Activation-aware Weight Quantization)</h2>
<p><em>Exploring hardware-optimized quantization designed for inference speed.</em></p>
<h3 id="awq-advantages">AWQ Advantages</h3>
<p>AWQ offers unique benefits:</p>
<ul>
<li><strong>Hardware Optimized</strong>: Designed for efficient GPU inference</li>
<li><strong>Activation Aware</strong>: Considers activation patterns during quantization</li>
<li><strong>Speed Focused</strong>: Minimal performance overhead</li>
<li><strong>vLLM Integration</strong>: Out-of-the-box support in production inference engines</li>
</ul>
<h3 id="results-2">Results</h3>



































<table><thead><tr><th>Metric</th><th>Value</th><th>vs FP16</th><th>vs BnB 4-bit</th></tr></thead><tbody><tr><td>GPU Memory Used</td><td>22,686MB (92.4% of VRAM)</td><td>+0.2%</td><td>+194%</td></tr><tr><td>Load Time</td><td>10.4s</td><td>35% faster</td><td>-58%</td></tr><tr><td>Tokens/Second</td><td>579.1</td><td>+70%</td><td>+1265%</td></tr><tr><td>Peak GPU Utilization</td><td>99.5%</td><td>+0.3%</td><td>+159%</td></tr></tbody></table>
<h3 id="performance-analysis">Performance Analysis</h3>
<p>AWQ delivers surprising results that challenge conventional assumptions about quantization:</p>
<p><strong>Unexpected Memory Usage</strong>: Despite being 4-bit quantized, AWQ uses nearly identical memory to FP16 (22.7GB vs 22.6GB). This suggests the pre-quantized model includes additional metadata or the quantization benefits are offset by implementation overhead.</p>
<p><strong>Superior Performance</strong>: AWQ excels in inference speed, delivering 579.1 tokens/second - a 70% improvement over FP16 and a massive 1265% improvement over BitsAndBytes. This demonstrates the hardware optimization focus of AWQ.</p>
<p><strong>Fast Loading</strong>: Model loading is 35% faster than FP16, indicating efficient initialization of the pre-quantized weights.</p>
<h2 id="experiment-4-gptq-quantization">Experiment 4: GPTQ Quantization</h2>
<p><em>Testing the established post-training quantization method.</em></p>
<h3 id="gptq-characteristics">GPTQ Characteristics</h3>
<p>GPTQ provides:</p>
<ul>
<li><strong>Mature Implementation</strong>: Well-established quantization method</li>
<li><strong>Good Compression</strong>: Effective weight compression</li>
<li><strong>Quality Trade-offs</strong>: Balanced quality preservation</li>
<li><strong>Broad Support</strong>: Available across multiple frameworks</li>
</ul>
<h3 id="results-3">Results</h3>








































<table><thead><tr><th>Metric</th><th>Value</th><th>vs FP16</th><th>vs AWQ</th><th>vs BnB 4-bit</th></tr></thead><tbody><tr><td>GPU Memory Used</td><td>22,689MB (92.4% of VRAM)</td><td>+0.3%</td><td>+0.0%</td><td>+194%</td></tr><tr><td>Load Time</td><td>21.0s</td><td>-31%</td><td>-101%</td><td>-219%</td></tr><tr><td>Tokens/Second</td><td>598.7</td><td>+76%</td><td>+3%</td><td>+1312%</td></tr><tr><td>Peak GPU Utilization</td><td>99.5%</td><td>+0.3%</td><td>+0.0%</td><td>+159%</td></tr></tbody></table>
<p><strong>Key Findings:</strong></p>
<p>GPTQ shows similar patterns to AWQ with some notable differences:</p>
<p><strong>Memory Usage</strong>: Like AWQ, GPTQ uses nearly full VRAM (22.7GB) despite 4-bit quantization, suggesting these pre-quantized models don’t deliver the expected memory savings in vLLM.</p>
<p><strong>Peak Performance</strong>: GPTQ achieves the highest inference speed at 598.7 tokens/second, slightly outperforming AWQ and delivering 76% better performance than FP16.</p>
<p><strong>Slower Loading</strong>: GPTQ takes significantly longer to load (21.0s vs 16.1s for FP16), likely due to model preprocessing or initialization overhead.</p>
<h2 id="comparative-analysis">Comparative Analysis</h2>
<h3 id="memory-efficiency-comparison">Memory Efficiency Comparison</h3>
<p>[Insert comprehensive comparison chart showing memory usage across all methods]</p>
<h3 id="performance-trade-offs">Performance Trade-offs</h3>













































<table><thead><tr><th>Method</th><th>Memory Usage</th><th>Memory Savings</th><th>Speed</th><th>Load Time</th><th>Ease of Use</th></tr></thead><tbody><tr><td>FP16 Baseline</td><td>22.6GB (92%)</td><td>-</td><td>339 t/s</td><td>16.1s</td><td>⭐⭐⭐⭐⭐</td></tr><tr><td>BitsAndBytes 4-bit</td><td>7.7GB (31%)</td><td><strong>66%</strong></td><td>42 t/s</td><td>6.6s</td><td>⭐⭐⭐⭐⭐</td></tr><tr><td>AWQ</td><td>22.7GB (92%)</td><td>0%</td><td><strong>579</strong> t/s</td><td>10.4s</td><td>⭐⭐⭐⭐</td></tr><tr><td>GPTQ</td><td>22.7GB (92%)</td><td>0%</td><td><strong>599</strong> t/s</td><td>21.0s</td><td>⭐⭐⭐</td></tr></tbody></table>
<h3 id="real-world-scenarios">Real-World Scenarios</h3>
<h4 id="scenario-1-interactive-chat-application">Scenario 1: Interactive Chat Application</h4>
<p><strong>Requirements</strong>: Low latency, moderate context length, good quality
<strong>Recommended</strong>: <strong>GPTQ</strong> - Delivers the highest inference speed (599 t/s) for responsive user interactions. While it uses full VRAM, the performance benefit justifies the memory cost for single-user scenarios.</p>
<h4 id="scenario-2-batch-processing">Scenario 2: Batch Processing</h4>
<p><strong>Requirements</strong>: High throughput, cost efficiency, acceptable quality
<strong>Recommended</strong>: <strong>AWQ</strong> - Offers excellent throughput (579 t/s) with faster loading than GPTQ. The slight speed difference is offset by better operational characteristics for batch workloads.</p>
<h4 id="scenario-3-long-document-analysis">Scenario 3: Long Document Analysis</h4>
<p><strong>Requirements</strong>: Extended context, memory efficiency, quality preservation
<strong>Recommended</strong>: <strong>BitsAndBytes 4-bit</strong> - Despite slower inference, the 66% memory reduction enables processing much longer documents (32K+ tokens) that wouldn’t fit with other methods. Quality remains excellent for analytical tasks.</p>
<h2 id="advanced-optimization-techniques">Advanced Optimization Techniques</h2>
<h3 id="vllm-configuration-tuning">vLLM Configuration Tuning</h3>
<p>Beyond quantization, several vLLM parameters significantly impact memory usage:</p>
<pre class="astro-code astro-code-themes min-light night-owl" style="background-color:#ffffff;--shiki-dark-bg:#011627;color:#24292eff;--shiki-dark:#d6deeb; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;" tabindex="0" data-language="yaml"><code><span class="line"><span style="color:#C2C3C5;--shiki-dark:#637777">#</span><span style="color:#C2C3C5;font-style:inherit;--shiki-dark:#637777;--shiki-dark-font-style:italic"> Memory-optimized configuration</span></span>
<span class="line"><span style="color:#D32F2F;--shiki-dark:#7FDBCA">memory_optimized</span><span style="color:#D32F2F;--shiki-dark:#D6DEEB">:</span></span>
<span class="line"><span style="color:#D32F2F;--shiki-dark:#7FDBCA">  gpu_memory_utilization</span><span style="color:#D32F2F;--shiki-dark:#D6DEEB">:</span><span style="color:#1976D2;--shiki-dark:#F78C6C"> 0.95</span></span>
<span class="line"><span style="color:#D32F2F;--shiki-dark:#7FDBCA">  max_model_len</span><span style="color:#D32F2F;--shiki-dark:#D6DEEB">:</span><span style="color:#1976D2;--shiki-dark:#F78C6C"> 2048</span></span>
<span class="line"><span style="color:#D32F2F;--shiki-dark:#7FDBCA">  swap_space</span><span style="color:#D32F2F;--shiki-dark:#D6DEEB">:</span><span style="color:#1976D2;--shiki-dark:#F78C6C"> 8</span></span>
<span class="line"><span style="color:#D32F2F;--shiki-dark:#7FDBCA">  cpu_offload_gb</span><span style="color:#D32F2F;--shiki-dark:#D6DEEB">:</span><span style="color:#1976D2;--shiki-dark:#F78C6C"> 2</span></span>
<span class="line"><span style="color:#D32F2F;--shiki-dark:#7FDBCA">  block_size</span><span style="color:#D32F2F;--shiki-dark:#D6DEEB">:</span><span style="color:#1976D2;--shiki-dark:#F78C6C"> 8</span></span>
<span class="line"><span style="color:#D32F2F;--shiki-dark:#7FDBCA">  max_num_seqs</span><span style="color:#D32F2F;--shiki-dark:#D6DEEB">:</span><span style="color:#1976D2;--shiki-dark:#F78C6C"> 64</span></span>
<span class="line"></span></code></pre>
<h3 id="system-level-optimizations">System-Level Optimizations</h3>
<ol>
<li>
<p><strong>CUDA Memory Management</strong></p>
<ul>
<li>Pre-allocate GPU memory</li>
<li>Disable memory fragmentation</li>
<li>Optimize CUDA context switching</li>
</ul>
</li>
<li>
<p><strong>Operating System Tuning</strong></p>
<ul>
<li>Increase virtual memory</li>
<li>Optimize page file settings</li>
<li>Configure GPU scheduling mode</li>
</ul>
</li>
<li>
<p><strong>Hardware Considerations</strong></p>
<ul>
<li>PCIe bandwidth optimization</li>
<li>CPU-GPU data transfer minimization</li>
<li>Thermal management</li>
</ul>
</li>
</ol>
<h2 id="my-learnings">My learnings</h2>
<h3 id="key-findings">Key Findings</h3>
<ol>
<li>
<p><strong>Memory vs Quality Trade-offs</strong>: BitsAndBytes is the only method that delivers significant memory savings (66% reduction), making it essential for memory-constrained scenarios despite the performance penalty.</p>
</li>
<li>
<p><strong>Performance vs Memory Trade-off</strong>: AWQ and GPTQ deliver 70-76% better performance than FP16 but use identical memory (22.7GB vs 22.6GB). This counterintuitive result suggests these “4-bit” models are optimized for speed in vLLM’s implementation, trading memory savings for performance gains.</p>
</li>
<li>
<p><strong>Practical Considerations</strong>: The choice between methods depends heavily on your bottleneck - memory constraints favor BitsAndBytes, while performance requirements favor AWQ/GPTQ.</p>
</li>
</ol>
<h3 id="surprising-results">Surprising Results</h3>
<p><strong>The Quantization Paradox</strong>: AWQ and GPTQ 4-bit models used identical memory to FP16 (22.7GB vs 22.6GB) while delivering superior performance. This challenges the assumption that quantization always reduces memory usage and suggests:</p>
<ul>
<li>Pre-quantized models may include additional metadata</li>
<li>vLLM’s implementation optimizes for speed over memory savings</li>
<li>The “4-bit” designation refers to weight storage, not runtime memory</li>
</ul>
<h2 id="possible-recommendations">Possible recommendations</h2>
<h3 id="for-different-use-cases">For Different Use Cases</h3>
<p><strong>Development and Experimentation</strong>:</p>
<ul>
<li><strong>Start with BitsAndBytes 4-bit</strong> for maximum memory headroom and experimentation flexibility</li>
<li>Use conservative memory settings (gpu_memory_utilization=0.7)</li>
<li>Monitor GPU temperature and usage with the provided monitoring tools</li>
</ul>
<p><strong>Production Deployment</strong>:</p>
<ul>
<li><strong>GPTQ for speed-critical applications</strong> - 599 tokens/sec with proven stability</li>
<li><strong>AWQ for throughput-focused workloads</strong> - 579 tokens/sec with faster loading</li>
<li><strong>BitsAndBytes for memory-critical deployments</strong> - only option for extended context lengths</li>
<li>Implement proper monitoring, error handling, and graceful degradation</li>
</ul>
<p><strong>Resource-Constrained Scenarios</strong>:</p>
<ul>
<li><strong>BitsAndBytes is your only viable option</strong> for significant memory reduction</li>
<li>Consider CPU offloading configurations for extreme cases</li>
<li>Optimize context length based on actual requirements rather than maximums</li>
</ul>
<h3 id="future-considerations">Future Considerations</h3>
<ol>
<li><strong>Emerging Quantization Methods</strong>: [What’s coming next]</li>
<li><strong>Hardware Evolution</strong>: [Next-gen GPU considerations]</li>
<li><strong>Framework Improvements</strong>: [Expected software advances]</li>
</ol>
<h2 id="conclusion">Conclusion</h2>
<p>Running Llama 8B models on a single RTX 4090 reveals surprising insights about modern quantization techniques. My comprehensive benchmarking shows that the choice of method depends critically on your primary constraint:</p>
<p><strong>For Memory-Constrained Scenarios</strong>: BitsAndBytes 4-bit is the clear winner, delivering 66% memory reduction (22.6GB → 7.7GB) with acceptable quality preservation, despite 87% slower inference.</p>
<p><strong>For Performance-Critical Applications</strong>: GPTQ achieves the highest throughput at 599 tokens/second, while AWQ offers similar performance (579 t/s) with better loading characteristics.</p>
<p><strong>The Quantization Paradox</strong>: My most surprising finding is that pre-quantized AWQ and GPTQ models use identical memory to FP16 while delivering 70-76% better performance. This challenges conventional wisdom about quantization being primarily a memory optimization technique.</p>
<p><strong>Key Takeaway</strong>: Modern quantization is more nuanced than expected. BitsAndBytes remains essential for memory optimization, while AWQ/GPTQ excel as performance optimizations rather than memory savers. Understanding these trade-offs enables informed decisions for your specific deployment requirements.</p>
<h2 id="appendix">Appendix</h2>
<h3 id="complete-benchmark-results">Complete Benchmark Results</h3>



































































































































<table><thead><tr><th>Metric</th><th>FP16 Baseline</th><th>BitsAndBytes 4-bit</th><th>AWQ</th><th>GPTQ</th></tr></thead><tbody><tr><td><strong>Memory Performance</strong></td><td></td><td></td><td></td><td></td></tr><tr><td>GPU Memory Used</td><td>22,631MB</td><td>7,711MB</td><td>22,686MB</td><td>22,689MB</td></tr><tr><td>GPU Utilization</td><td>99.2%</td><td>38.5%</td><td>99.5%</td><td>99.5%</td></tr><tr><td>Memory Savings vs FP16</td><td>-</td><td>66.0%</td><td>0.0%</td><td>0.0%</td></tr><tr><td>Free VRAM</td><td>196MB</td><td>15,110MB</td><td>124MB</td><td>121MB</td></tr><tr><td><strong>Performance Metrics</strong></td><td></td><td></td><td></td><td></td></tr><tr><td>Model Load Time</td><td>16.1s</td><td>6.6s</td><td>10.4s</td><td>21.0s</td></tr><tr><td>Inference Speed</td><td>339.6 t/s</td><td>42.4 t/s</td><td>579.1 t/s</td><td>598.7 t/s</td></tr><tr><td>Speed vs FP16</td><td>-</td><td>-87.5%</td><td>+70.5%</td><td>+76.3%</td></tr><tr><td>Total Tokens Generated</td><td>1,792</td><td>1,799</td><td>1,792</td><td>1,792</td></tr><tr><td><strong>System Resources</strong></td><td></td><td></td><td></td><td></td></tr><tr><td>System RAM Increase</td><td>1,024MB</td><td>163MB</td><td>1,062MB</td><td>1,385MB</td></tr><tr><td>System RAM %</td><td>9.6%</td><td>9.0%</td><td>9.7%</td><td>10.6%</td></tr><tr><td><strong>Configuration</strong></td><td></td><td></td><td></td><td></td></tr><tr><td>Model Source</td><td>HuggingFace</td><td>HuggingFace</td><td>Pre-quantized</td><td>Pre-quantized</td></tr><tr><td>Quantization Bits</td><td>16</td><td>4 (NF4)</td><td>4</td><td>4</td></tr><tr><td>Framework</td><td>vLLM</td><td>Transformers+BnB</td><td>vLLM</td><td>vLLM</td></tr></tbody></table>
<h3 id="reproducibility">Reproducibility</h3>
<p>All experiments in this analysis are fully reproducible using the provided <a href="https://github.com/ermolushka/vllm-benchmark">code</a></p>
<hr>
<p><em>This analysis was conducted using automated benchmarking tools. All performance measurements are specific to the tested hardware configuration and may vary on different systems.</em></p> </article> <ul class="my-8 astro-vj4tpspi"> <li class="inline-block my-1 underline-offset-4 astro-blwjyjpt"> <a href="/tags/v-llm/" class="text-sm pr-2 group astro-blwjyjpt" data-astro-transition-scope="astro-36ssibgs-2"> <svg xmlns="http://www.w3.org/2000/svg" class=" scale-75 astro-blwjyjpt"><path d="M16.018 3.815 15.232 8h-4.966l.716-3.815-1.964-.37L8.232 8H4v2h3.857l-.751 4H3v2h3.731l-.714 3.805 1.965.369L8.766 16h4.966l-.714 3.805 1.965.369.783-4.174H20v-2h-3.859l.751-4H21V8h-3.733l.716-3.815-1.965-.37zM14.106 14H9.141l.751-4h4.966l-.752 4z" class="astro-blwjyjpt"></path> </svg>
&nbsp;<span class="astro-blwjyjpt">v-llm</span> </a> </li> <li class="inline-block my-1 underline-offset-4 astro-blwjyjpt"> <a href="/tags/ml-inference/" class="text-sm pr-2 group astro-blwjyjpt" data-astro-transition-scope="astro-36ssibgs-3"> <svg xmlns="http://www.w3.org/2000/svg" class=" scale-75 astro-blwjyjpt"><path d="M16.018 3.815 15.232 8h-4.966l.716-3.815-1.964-.37L8.232 8H4v2h3.857l-.751 4H3v2h3.731l-.714 3.805 1.965.369L8.766 16h4.966l-.714 3.805 1.965.369.783-4.174H20v-2h-3.859l.751-4H21V8h-3.733l.716-3.815-1.965-.37zM14.106 14H9.141l.751-4h4.966l-.752 4z" class="astro-blwjyjpt"></path> </svg>
&nbsp;<span class="astro-blwjyjpt">ml-inference</span> </a> </li> <li class="inline-block my-1 underline-offset-4 astro-blwjyjpt"> <a href="/tags/cuda/" class="text-sm pr-2 group astro-blwjyjpt" data-astro-transition-scope="astro-36ssibgs-4"> <svg xmlns="http://www.w3.org/2000/svg" class=" scale-75 astro-blwjyjpt"><path d="M16.018 3.815 15.232 8h-4.966l.716-3.815-1.964-.37L8.232 8H4v2h3.857l-.751 4H3v2h3.731l-.714 3.805 1.965.369L8.766 16h4.966l-.714 3.805 1.965.369.783-4.174H20v-2h-3.859l.751-4H21V8h-3.733l.716-3.815-1.965-.37zM14.106 14H9.141l.751-4h4.966l-.752 4z" class="astro-blwjyjpt"></path> </svg>
&nbsp;<span class="astro-blwjyjpt">cuda</span> </a> </li> <li class="inline-block my-1 underline-offset-4 astro-blwjyjpt"> <a href="/tags/gpu/" class="text-sm pr-2 group astro-blwjyjpt" data-astro-transition-scope="astro-36ssibgs-5"> <svg xmlns="http://www.w3.org/2000/svg" class=" scale-75 astro-blwjyjpt"><path d="M16.018 3.815 15.232 8h-4.966l.716-3.815-1.964-.37L8.232 8H4v2h3.857l-.751 4H3v2h3.731l-.714 3.805 1.965.369L8.766 16h4.966l-.714 3.805 1.965.369.783-4.174H20v-2h-3.859l.751-4H21V8h-3.733l.716-3.815-1.965-.37zM14.106 14H9.141l.751-4h4.966l-.752 4z" class="astro-blwjyjpt"></path> </svg>
&nbsp;<span class="astro-blwjyjpt">gpu</span> </a> </li> <li class="inline-block my-1 underline-offset-4 astro-blwjyjpt"> <a href="/tags/llama/" class="text-sm pr-2 group astro-blwjyjpt" data-astro-transition-scope="astro-36ssibgs-6"> <svg xmlns="http://www.w3.org/2000/svg" class=" scale-75 astro-blwjyjpt"><path d="M16.018 3.815 15.232 8h-4.966l.716-3.815-1.964-.37L8.232 8H4v2h3.857l-.751 4H3v2h3.731l-.714 3.805 1.965.369L8.766 16h4.966l-.714 3.805 1.965.369.783-4.174H20v-2h-3.859l.751-4H21V8h-3.733l.716-3.815-1.965-.37zM14.106 14H9.141l.751-4h4.966l-.752 4z" class="astro-blwjyjpt"></path> </svg>
&nbsp;<span class="astro-blwjyjpt">llama</span> </a> </li> <li class="inline-block my-1 underline-offset-4 astro-blwjyjpt"> <a href="/tags/quantization/" class="text-sm pr-2 group astro-blwjyjpt" data-astro-transition-scope="astro-36ssibgs-7"> <svg xmlns="http://www.w3.org/2000/svg" class=" scale-75 astro-blwjyjpt"><path d="M16.018 3.815 15.232 8h-4.966l.716-3.815-1.964-.37L8.232 8H4v2h3.857l-.751 4H3v2h3.731l-.714 3.805 1.965.369L8.766 16h4.966l-.714 3.805 1.965.369.783-4.174H20v-2h-3.859l.751-4H21V8h-3.733l.716-3.815-1.965-.37zM14.106 14H9.141l.751-4h4.966l-.752 4z" class="astro-blwjyjpt"></path> </svg>
&nbsp;<span class="astro-blwjyjpt">quantization</span> </a> </li>  </ul> <div class="flex flex-col-reverse items-center justify-between gap-6 sm:flex-row-reverse sm:items-end sm:gap-4 astro-vj4tpspi"> <button id="back-to-top" class="focus-outline whitespace-nowrap py-1 hover:opacity-75 astro-vj4tpspi"> <svg xmlns="http://www.w3.org/2000/svg" class="rotate-90 astro-vj4tpspi"> <path d="M13.293 6.293 7.586 12l5.707 5.707 1.414-1.414L10.414 12l4.293-4.293z" class="astro-vj4tpspi"></path> </svg> <span class="astro-vj4tpspi">Back to Top</span> </button> <div class="social-icons astro-wkojbtzc"> <span class="italic astro-wkojbtzc">Share this post on:</span> <div class="text-center astro-wkojbtzc"> <a href="https://wa.me/?text=https://ermolushka.github.io/posts/vllm-benchmark-4090/" class="group inline-block hover:text-skin-accent link-button astro-wkojbtzc" title="Share this post via WhatsApp"> <svg
      xmlns="http://www.w3.org/2000/svg"
      class="icon-tabler"
      stroke-linecap="round"
      stroke-linejoin="round"
    >
      <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3 21l1.65 -3.8a9 9 0 1 1 3.4 2.9l-5.05 .9"></path>
      <path d="M9 10a0.5 .5 0 0 0 1 0v-1a0.5 .5 0 0 0 -1 0v1a5 5 0 0 0 5 5h1a0.5 .5 0 0 0 0 -1h-1a0.5 .5 0 0 0 0 1"></path>
    </svg> <span class="sr-only astro-wkojbtzc">Share this post via WhatsApp</span> </a><a href="https://www.facebook.com/sharer.php?u=https://ermolushka.github.io/posts/vllm-benchmark-4090/" class="group inline-block hover:text-skin-accent link-button astro-wkojbtzc" title="Share this post on Facebook"> <svg
    xmlns="http://www.w3.org/2000/svg"
    class="icon-tabler"
    stroke-linecap="round"
    stroke-linejoin="round"
  >
    <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
    <path
      d="M7 10v4h3v7h4v-7h3l1 -4h-4v-2a1 1 0 0 1 1 -1h3v-4h-3a5 5 0 0 0 -5 5v2h-3"
    ></path>
  </svg> <span class="sr-only astro-wkojbtzc">Share this post on Facebook</span> </a><a href="https://x.com/intent/post?url=https://ermolushka.github.io/posts/vllm-benchmark-4090/" class="group inline-block hover:text-skin-accent link-button astro-wkojbtzc" title="Share this post on X"> <svg  
      xmlns="http://www.w3.org/2000/svg"
      class="icon-tabler"
      stroke-linecap="round"
      stroke-linejoin="round"
    >
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <path d="M4 4l11.733 16h4.267l-11.733 -16z" /><path d="M4 20l6.768 -6.768m2.46 -2.46l6.772 -6.772" />
    </svg> <span class="sr-only astro-wkojbtzc">Share this post on X</span> </a><a href="https://t.me/share/url?url=https://ermolushka.github.io/posts/vllm-benchmark-4090/" class="group inline-block hover:text-skin-accent link-button astro-wkojbtzc" title="Share this post via Telegram"> <svg
        xmlns="http://www.w3.org/2000/svg"
        class="icon-tabler"
        stroke-linecap="round"
        stroke-linejoin="round"
      >
        <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
        <path d="M15 10l-4 4l6 6l4 -16l-18 7l4 2l2 6l3 -4"></path>
      </svg> <span class="sr-only astro-wkojbtzc">Share this post via Telegram</span> </a><a href="https://pinterest.com/pin/create/button/?url=https://ermolushka.github.io/posts/vllm-benchmark-4090/" class="group inline-block hover:text-skin-accent link-button astro-wkojbtzc" title="Share this post on Pinterest"> <svg
      xmlns="http://www.w3.org/2000/svg"
      class="icon-tabler"
      stroke-linecap="round"
      stroke-linejoin="round"
    >
      <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
      <line x1="8" y1="20" x2="12" y2="11"></line>
      <path d="M10.7 14c.437 1.263 1.43 2 2.55 2c2.071 0 3.75 -1.554 3.75 -4a5 5 0 1 0 -9.7 1.7"></path>
      <circle cx="12" cy="12" r="9"></circle>
    </svg> <span class="sr-only astro-wkojbtzc">Share this post on Pinterest</span> </a><a href="mailto:?subject=See%20this%20post&#38;body=https://ermolushka.github.io/posts/vllm-benchmark-4090/" class="group inline-block hover:text-skin-accent link-button astro-wkojbtzc" title="Share this post via email"> <svg
      xmlns="http://www.w3.org/2000/svg"
      class="icon-tabler"
      stroke-linecap="round"
      stroke-linejoin="round"
    >
      <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
      <rect x="3" y="5" width="18" height="14" rx="2"></rect>
      <polyline points="3 7 12 13 21 7"></polyline>
    </svg> <span class="sr-only astro-wkojbtzc">Share this post via email</span> </a> </div> </div>  </div> <hr class="my-6 border-dashed astro-vj4tpspi"> <!-- Previous/Next Post Buttons --> <div class="grid grid-cols-1 gap-6 sm:grid-cols-2 astro-vj4tpspi">  <a href="/posts/data-capture-ml-endpoints" class="flex w-full justify-end gap-1 text-right hover:opacity-75 sm:col-start-2 astro-vj4tpspi"> <div class="astro-vj4tpspi"> <span class="astro-vj4tpspi">Next Post</span> <div class="text-sm text-skin-accent/85 astro-vj4tpspi">Data capture for ML endpoints</div> </div> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-chevron-right flex-none astro-vj4tpspi">  <path stroke="none" d="M0 0h24v24H0z" fill="none" class="astro-vj4tpspi"></path> <path d="M9 6l6 6l-6 6" class="astro-vj4tpspi"></path>  </svg> </a> </div> </main> <footer class="mt-auto astro-sz7xmlte"> <div class="max-w-3xl mx-auto px-0"> <hr class="border-skin-line" aria-hidden="true"> </div> <div class="footer-wrapper astro-sz7xmlte"> <div class="social-icons flex astro-upu6fzxr"> <a href="https://github.com/ermolushka" class="group inline-block hover:text-skin-accent link-button astro-upu6fzxr" title=" ermolushka (blog) on Github"> <svg
    xmlns="http://www.w3.org/2000/svg"
    class="icon-tabler"
    stroke-linecap="round"
    stroke-linejoin="round"
  >
    <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
    <path
      d="M9 19c-4.3 1.4 -4.3 -2.5 -6 -3m12 5v-3.5c0 -1 .1 -1.4 -.5 -2c2.8 -.3 5.5 -1.4 5.5 -6a4.6 4.6 0 0 0 -1.3 -3.2a4.2 4.2 0 0 0 -.1 -3.2s-1.1 -.3 -3.5 1.3a12.3 12.3 0 0 0 -6.2 0c-2.4 -1.6 -3.5 -1.3 -3.5 -1.3a4.2 4.2 0 0 0 -.1 3.2a4.6 4.6 0 0 0 -1.3 3.2c0 4.6 2.7 5.7 5.5 6c-.6 .6 -.6 1.2 -.5 2v3.5"
    ></path>
  </svg> <span class="sr-only astro-upu6fzxr"> ermolushka (blog) on Github</span> </a> </div>  <div class="copyright-wrapper astro-sz7xmlte"> <span class="astro-sz7xmlte">Copyright &#169; 2025</span> <span class="separator astro-sz7xmlte">&nbsp;|&nbsp;</span> <span class="astro-sz7xmlte">All rights reserved.</span> </div> </div> </footer>   </body></html>  <script data-astro-rerun>
  /** Create a progress indicator
   *  at the top */
  function createProgressBar() {
    // Create the main container div
    const progressContainer = document.createElement("div");
    progressContainer.className =
      "progress-container fixed top-0 z-10 h-1 w-full bg-skin-fill";

    // Create the progress bar div
    const progressBar = document.createElement("div");
    progressBar.className = "progress-bar h-1 w-0 bg-skin-accent";
    progressBar.id = "myBar";

    // Append the progress bar to the progress container
    progressContainer.appendChild(progressBar);

    // Append the progress container to the document body or any other desired parent element
    document.body.appendChild(progressContainer);
  }
  createProgressBar();

  /** Update the progress bar
   *  when user scrolls */
  function updateScrollProgress() {
    document.addEventListener("scroll", () => {
      const winScroll =
        document.body.scrollTop || document.documentElement.scrollTop;
      const height =
        document.documentElement.scrollHeight -
        document.documentElement.clientHeight;
      const scrolled = (winScroll / height) * 100;
      if (document) {
        const myBar = document.getElementById("myBar");
        if (myBar) {
          myBar.style.width = scrolled + "%";
        }
      }
    });
  }
  updateScrollProgress();

  /** Attaches links to headings in the document,
   *  allowing sharing of sections easily */
  function addHeadingLinks() {
    const headings = Array.from(
      document.querySelectorAll("h2, h3, h4, h5, h6"),
    );
    for (const heading of headings) {
      heading.classList.add("group");
      const link = document.createElement("a");
      link.className =
        "heading-link ml-2 opacity-0 group-hover:opacity-100 focus:opacity-100";
      link.href = "#" + heading.id;

      const span = document.createElement("span");
      span.ariaHidden = "true";
      span.innerText = "#";
      link.appendChild(span);
      heading.appendChild(link);
    }
  }
  addHeadingLinks();

  /** Attaches copy buttons to code blocks in the document,
   * allowing users to copy code easily. */
  function attachCopyButtons() {
    const copyButtonLabel = "Copy";
    const codeBlocks = Array.from(document.querySelectorAll("pre"));

    for (const codeBlock of codeBlocks) {
      const wrapper = document.createElement("div");
      wrapper.style.position = "relative";

      const copyButton = document.createElement("button");
      copyButton.className =
        "copy-code absolute right-3 -top-3 rounded bg-skin-card px-2 py-1 text-xs leading-4 text-skin-base font-medium";
      copyButton.innerHTML = copyButtonLabel;
      codeBlock.setAttribute("tabindex", "0");
      codeBlock.appendChild(copyButton);

      // wrap codebock with relative parent element
      codeBlock?.parentNode?.insertBefore(wrapper, codeBlock);
      wrapper.appendChild(codeBlock);

      copyButton.addEventListener("click", async () => {
        await copyCode(codeBlock, copyButton);
      });
    }

    async function copyCode(block, button) {
      const code = block.querySelector("code");
      const text = code?.innerText;

      await navigator.clipboard.writeText(text ?? "");

      // visual feedback that task is completed
      button.innerText = "Copied";

      setTimeout(() => {
        button.innerText = copyButtonLabel;
      }, 700);
    }
  }
  attachCopyButtons();

  /** Scrolls the document to the top when
   * the "Back to Top" button is clicked. */
  function backToTop() {
    document.querySelector("#back-to-top")?.addEventListener("click", () => {
      document.body.scrollTop = 0; // For Safari
      document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
    });
  }
  backToTop();

  /* Go to page start after page swap */
  document.addEventListener("astro:after-swap", () =>
    window.scrollTo({ left: 0, top: 0, behavior: "instant" }),
  );
</script>